# ðŸŽµ Julia Wolf Streaming Analytics Pipeline

A lightweight data pipeline that simulates real-time music streaming ingestion
and aggregation â€” built as a portfolio project demonstrating backend/data
engineering fundamentals.

---

## What It Does

Simulates a Kafka-style event stream of Julia Wolf track plays, processes them
through a stateful aggregation layer, and outputs ranked analytics â€” the same
pattern used in production ad-tech and music platforms.

```
EventProducer  â”€â”€â–¶  StreamProcessor  â”€â”€â–¶  Report + JSON Snapshot
 (synthetic        (windowed stats,        (leaderboard, deep-
  Kafka topic)      enrichment join)        dive, throughput)
```

**Three simulated "waves"** model real traffic patterns:
- ðŸš€ New release spike
- ðŸ“ˆ Weekend surge
- ðŸ“Š Steady-state baseline

---

## Pipeline Concepts Demonstrated

| Concept | Implementation |
|---|---|
| Event streaming | `EventProducer` emits `StreamEvent` dataclass objects |
| Windowed aggregation | Tumbling 60-second window for throughput calculation |
| Enrichment join | Events enriched with catalog metadata at ingest time |
| Cardinality estimation | Unique user count via in-memory set (HLL-ready pattern) |
| Weighted sampling | Track popularity weights via `random.choices` |
| Batch processing | `process_batch()` for bulk ingest simulation |
| JSON export | Snapshot serialized via `dataclasses.asdict` |

---

## Tech Stack

- **Python 3.11+** â€” standard library only, zero external dependencies
- Patterns map to production tools: `EventProducer` â†’ Kafka, `StreamProcessor` â†’ Flink/Spark Streaming, JSON output â†’ S3/BigQuery sink

---

## Run It

```bash
python pipeline.py
```

Expected output:
```
Starting Julia Wolf Streaming Analytics Pipeline...
Simulating event ingestion in 3 waves...

  ðŸš€ Wave 1 â€” New Release Spike   (1,200 events)... done
  ðŸ“ˆ Wave 2 â€” Weekend Surge        (2,500 events)... done
  ðŸ“Š Wave 3 â€” Steady State Baseline  (800 events)... done

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ðŸŽµ  Julia Wolf Â· Streaming Analytics Report
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pipeline Stats
  Total events processed : 4,500
  Unique tracks streamed : 8
  Throughput (last 60s)  : X.XX events/sec

Track Leaderboard  (all time)
  ðŸ¥‡ Medicine           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  998 streams  complete: 85.2%  top: Apple Music
  ðŸ¥ˆ Falling for U      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  812 streams  ...
  ...
```

---

## Extension Ideas

- Swap `EventProducer` for a real Kafka consumer (`confluent-kafka-python`)
- Persist `TrackStats` to Redis for shared state across workers
- Add a Flink-style watermark to handle out-of-order events
- Expose `/metrics` endpoint via FastAPI for Prometheus scraping
- Replace set-based unique count with a true HyperLogLog (Redis `PFADD`)

---

## Why This Project

Ad-tech and streaming platforms process billions of events daily with the same
core patterns shown here: enrichment joins, windowed aggregation, and
cardinality estimation at scale. This project demonstrates familiarity with
those fundamentals in a concrete, runnable form.
